version: '3.5'

services:
  namenode:
    image: namenode:hadoop-3.4.0 
    container_name: namenode
    restart: always
    ports:
      - 9870:9870
      - 9000:9000
    volumes:
      - hadoop_namenode:/hadoop/dfs/name
    environment:
      - CLUSTER_NAME=test
    env_file:
      - ./config
    networks:
      - etl_task
    # command: ["hdfs", "namenode", "-upgrade"]
    
  datanode:
    image: datanode:hadoop-3.4.0 
    container_name: datanode
    restart: always
    volumes:
      - hadoop_datanode:/hadoop/dfs/data
    environment:
      SERVICE_PRECONDITION: "namenode:9870"
    env_file:
      - ./config
    networks:
      - etl_task
  # datanode2:
  #   image: datanode:hadoop-3.4.0 
  #   container_name: datanode2
  #   restart: always
  #   volumes:
  #     - hadoop_datanode2:/hadoop/dfs/data
  #   environment:
  #     SERVICE_PRECONDITION: "namenode:9870"
  #   env_file:
  #     - ./config
  #   networks:
  #     - etl_task
  # datanode3:
  #   image: datanode:hadoop-3.4.0 
  #   container_name: datanode3
  #   restart: always
  #   volumes:
  #     - hadoop_datanode3:/hadoop/dfs/data
  #   environment:
  #     SERVICE_PRECONDITION: "namenode:9870"
  #   env_file:
  #     - ./config
  #   networks:
  #     - etl_task
    
  spark:
    image: docker.io/bitnami/spark:3.5.2
    environment:
      - SPARK_MODE=master
      - SPARK_RPC_AUTHENTICATION_ENABLED=no
      - SPARK_RPC_ENCRYPTION_ENABLED=no
      - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
      - SPARK_SSL_ENABLED=no
      - SPARK_USER=spark
      - CORE_CONF_fs_defaultFS=hdfs://namenode:9000
      - CORE_CONF_fs_default_name=hdfs://namenode:9000
      - CORE_CONF_hadoop_http_staticuser_user=root
      - CORE_CONF_hadoop_proxyuser_hue_hosts=*
      - CORE_CONF_hadoop_proxyuser_hue_groups=*
      - CORE_CONF_io_compression_codecs=org.apache.hadoop.io.compress.SnappyCodec
      - CORE_CONF_ipc_maximum_data_length=134217728
      - HDFS_CONF_dfs_webhdfs_enabled=true
      - HDFS_CONF_dfs_permissions_enabled=false
      - HDFS_CONF_dfs_namenode_datanode_registration_ip___hostname___check=false
      - HDFS_CONF_dfs_replication=2
    ports:
      - '8080:8080'
      - '7077:7077'
    networks:
      - etl_task

  spark-worker:
    image: docker.io/bitnami/spark:3.5.2
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://spark:7077
      - SPARK_WORKER_MEMORY=1G
      - SPARK_WORKER_CORES=1
      - SPARK_RPC_AUTHENTICATION_ENABLED=no
      - SPARK_RPC_ENCRYPTION_ENABLED=no
      - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
      - SPARK_SSL_ENABLED=no
      - SPARK_USER=spark
      - CORE_CONF_fs_defaultFS=hdfs://namenode:9000
      - CORE_CONF_fs_default_name=hdfs://namenode:9000
      - CORE_CONF_hadoop_http_staticuser_user=root
      - CORE_CONF_hadoop_proxyuser_hue_hosts=*
      - CORE_CONF_hadoop_proxyuser_hue_groups=*
      - CORE_CONF_io_compression_codecs=org.apache.hadoop.io.compress.SnappyCodec
      - CORE_CONF_ipc_maximum_data_length=134217728
      - HDFS_CONF_dfs_webhdfs_enabled=true
      - HDFS_CONF_dfs_permissions_enabled=false
      - HDFS_CONF_dfs_namenode_datanode_registration_ip___hostname___check=false
      - HDFS_CONF_dfs_replication=2
    networks:
      - etl_task
    deploy:
      replicas: 1

  # jupyter:
  #   image: jupyter/pyspark-notebook:latest
  #   container_name: jupyter
  #   restart: always
  #   ports:
  #     - "8888:8888"
  #   environment:
  #     - PYSPARK_PYTHON=python3
  #     - PYSPARK_DRIVER_PYTHON=jupyter
  #     - PYSPARK_DRIVER_PYTHON_OPTS='notebook --ip=0.0.0.0 --port=8888 --no-browser --allow-root --NotebookApp.token='  # Disable token
  #     - SPARK_MASTER_URL=spark://spark:7077
  #   networks:
  #     - etl_task
  #   command: bash -c "pip install pyspark==3.5.2 && start-notebook.sh"
volumes:
  hadoop_namenode:
  hadoop_datanode:
  hadoop_datanode2:
  hadoop_datanode3:

networks:
  etl_task:
    name: custom_network

# ADD minio + gui https://github.com/josephmachado/efficient_data_processing_spark/blob/main/docker-compose.yml