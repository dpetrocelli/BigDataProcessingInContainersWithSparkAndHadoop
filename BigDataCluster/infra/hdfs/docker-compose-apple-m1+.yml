version: "3.5"

services:
  namenode:
    image: dpetrocelli/namenode-hadoop-3.4.0-arm
    container_name: namenode
    restart: always
    ports:
      - 9870:9870
      - 9000:9000
    volumes:
      - hadoop_namenode:/hadoop/dfs/name
    environment:
      - CLUSTER_NAME=test
    env_file:
      - ./config
    networks:
      - etl_task
    # command: ["hdfs", "namenode", "-upgrade"]
  datanode:
    image: dpetrocelli/datanode-hadoop-3.4.0-arm
    container_name: datanode
    restart: always
    volumes:
      - hadoop_datanode:/hadoop/dfs/data
    environment:
      SERVICE_PRECONDITION: "namenode:9870"
    env_file:
      - ./config
    networks:
      - etl_task

  spark:
    image: docker.io/bitnami/spark:3.5.2
    environment:
      - SPARK_MODE=master
      - SPARK_RPC_AUTHENTICATION_ENABLED=no
      - SPARK_RPC_ENCRYPTION_ENABLED=no
      - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
      - SPARK_SSL_ENABLED=no
      - SPARK_USER=spark
      - SPARK_CONF_DIR=/opt/bitnami/spark/conf
      - SPARK_SERIALIZER=org.apache.spark.serializer.KryoSerializer
      - CORE_CONF_fs_defaultFS=hdfs://namenode:9000
      - CORE_CONF_fs_default_name=hdfs://namenode:9000
      - CORE_CONF_hadoop_http_staticuser_user=root
      - CORE_CONF_hadoop_proxyuser_hue_hosts=*
      - CORE_CONF_hadoop_proxyuser_hue_groups=*
      - CORE_CONF_io_compression_codecs=org.apache.hadoop.io.compress.SnappyCodec
      - CORE_CONF_ipc_maximum_data_length=134217728
      - HDFS_CONF_dfs_webhdfs_enabled=true
      - HDFS_CONF_dfs_permissions_enabled=false
      - HDFS_CONF_dfs_namenode_datanode_registration_ip__hostname__check=false
      - HDFS_CONF_dfs_replication=2

    ports:
      - "8080:8080"
      - "7077:7077"
    networks:
      - etl_task

  spark-worker:
    image: docker.io/bitnami/spark:3.5.2
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://spark:7077
      - SPARK_WORKER_MEMORY=1G
      - SPARK_WORKER_CORES=1
      - SPARK_RPC_AUTHENTICATION_ENABLED=no
      - SPARK_RPC_ENCRYPTION_ENABLED=no
      - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
      - SPARK_SSL_ENABLED=no
      - SPARK_USER=spark
      - SPARK_CONF_DIR=/opt/bitnami/spark/conf
      - SPARK_SERIALIZER=org.apache.spark.serializer.KryoSerializer
      - CORE_CONF_fs_defaultFS=hdfs://namenode:9000
      - CORE_CONF_fs_default_name=hdfs://namenode:9000
      - CORE_CONF_hadoop_http_staticuser_user=root
      - CORE_CONF_hadoop_proxyuser_hue_hosts=*
      - CORE_CONF_hadoop_proxyuser_hue_groups=*
      - CORE_CONF_io_compression_codecs=org.apache.hadoop.io.compress.SnappyCodec
      - CORE_CONF_ipc_maximum_data_length=134217728
      - HDFS_CONF_dfs_webhdfs_enabled=true
      - HDFS_CONF_dfs_permissions_enabled=false
      - HDFS_CONF_dfs_namenode_datanode_registration_ip__hostname__check=false
      - HDFS_CONF_dfs_replication=2
    networks:
      - etl_task
    deploy:
      replicas: 1

  # jupyter:
  #   image: jupyter/pyspark-notebook:latest
  #   container_name: jupyter
  #   restart: always
  #   ports:
  #     - 8888:8888
  #   environment:
  #     - SPARK_MASTER=spark://spark:7077
  #     - PYSPARK_PYTHON=python
  #   networks:
  #     - etl_task
  #   volumes:
  #     - ../../data/notebooks:/home/david/work

  datascience-notebook:
    image: jupyter/pyspark-notebook:latest
    container_name: jupyter
    restart: always
    # volumes:
    #     - /tmp/jupyter_test_dir:/home/docker_worker/work
    ports:
      - 8888:8888
    environment:
      - SPARK_MASTER=spark://spark:7077
      - PYSPARK_PYTHON=python
    networks:
      - etl_task
    volumes:
      - ../../data/notebooks:/home/david/work
    command: >
      sh -c "pip install pyspark && start-notebook.sh --NotebookApp.token='' "

volumes:
  hadoop_namenode:
  hadoop_datanode:
  hadoop_datanode2:
  hadoop_datanode3:

networks:
  etl_task:
    name: custom_network
# ADD minio + gui https://github.com/josephmachado/efficient_data_processing_spark/blob/main/docker-compose.yml
