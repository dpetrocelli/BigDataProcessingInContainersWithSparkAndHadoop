#Run a MapReduce Job:
#You can use a sample MapReduce program provided by Hadoop. For example:
# if needed remove the folder. hdfs dfs -rm -r /user/david/output/
hdfs dfs -mkdir -p /user/david
hdfs dfs -put /tmp/wordcount.txt /user/david/

hadoop jar /opt/hadoop-3.4.0/share/hadoop/mapreduce/hadoop-mapreduce-examples-*.jar wordcount /user/david/wordcount.txt /user/david/output

hdfs dfs -ls /user/david/output
hdfs dfs -cat /user/david/output/part-r-00000

# STEP 1 - Resource Manager Connection:
# ------------------------------------------
    # -> INFO client.DefaultNoHARMFailoverProxyProvider: Connecting to ResourceManager...
    # --> which is responsible for allocating cluster resources.

# STEP 2 - Input Splits:
# ------------------------------------------
    # INFO input.FileInputFormat: Total input files to process: 1
    # INFO mapreduce.JobSubmitter: number of splits:1
    # Hadoop divides the input file (/user/david/wordcount.txt) into input splits for parallel processing. In this case, there is only one split because the file is small.

# STEP 3 - Job Submission:
# ------------------------------------------
    # INFO mapreduce.JobSubmitter: Submitting tokens for job...
    # INFO impl.YarnClientImpl: Submitted application...
    # The job is submitted to YARN, which manages resource allocation across the cluster.

# STEP 4 - Map Phase:
# ------------------------------------------
    # INFO mapreduce.Job: map 0% reduce 0%
    # INFO mapreduce.Job: map 100% reduce 0%
    # NOTE 
    # This shows the progress of the Map phase. The Map task reads the input split, processes it, and produces intermediate key-value pairs. Here, the mapping phase completed successfully.

# STEP 5 - Reduce Phase:
# ------------------------------------------
    # INFO mapreduce.Job: map 100% reduce 100%
    # NOTE 
    # After the Map phase, the Reduce phase aggregates the intermediate data. In this case, the Reduce task also completed successfully.

# Counters Explanation:
# ------------------------------------------
    # File System Counters:
        # These counters track file I/O operations, including bytes read/written in both the local filesystem and HDFS.

    # Job Counters:
        # These track the number of map/reduce tasks launched, time spent by tasks, and other resource usage.
        # Launched map tasks=1
        # Launched reduce tasks=1
        # Data-local map tasks=1
    
    # Map-Reduce Framework Counters:

        # Map input records: Number of records processed by the Mapper (4 records).
        # Map output records: Number of key-value pairs output by the Mapper (67 pairs).
        # Reduce input groups: Number of unique keys passed to the Reducer (48 groups).
        # Reduce output records: Number of final key-value pairs output by the Reducer (48 records).
        # Spilled Records: Number of records written to disk (96 records).
    
    # How the Process is Split and Joined:
        # Splitting (Map Phase):

            # The input data is split into chunks (in this case, only one chunk) and processed in parallel by Mapper tasks. Each Mapper processes its split independently and outputs key-value pairs.
            # Shuffling and Sorting:

            # The intermediate key-value pairs generated by the Mappers are shuffled (grouped by key) and sorted. This step ensures that all values associated with the same key are sent to the same Reducer.
    
    # Joining (Reduce Phase):

        # The Reducer receives the grouped key-value pairs, processes them, and produces the final output. The output is written to the specified output path in HDFS.